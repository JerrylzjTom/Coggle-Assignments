{"cells":[{"cell_type":"code","execution_count":50,"metadata":{"executionInfo":{"elapsed":4950,"status":"ok","timestamp":1715869868678,"user":{"displayName":"Zhijie Li","userId":"03138863041437547137"},"user_tz":-480},"id":"N4TQvoMKzcWa"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import pandas as pd\n","import jieba\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pprint\n","from gensim.models import Word2Vec\n","from torch.nn.utils.rnn import pad_sequence\n","from gensim.corpora.dictionary import Dictionary\n","from sklearn.model_selection import train_test_split\n","import warnings\n","from sklearn.preprocessing import LabelEncoder\n","warnings.simplefilter('ignore')\n"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["data_dir = 'https://mirror.coggle.club/dataset/coggle-competition/'\n","train_data = pd.read_csv(data_dir + 'intent-classify/train.csv', sep='\\t', header=None)\n","test_data = pd.read_csv(data_dir + 'intent-classify/test.csv', sep='\\t', header=None)\n","cn_stopwords = pd.read_csv('https://mirror.coggle.club/stopwords/baidu_stopwords.txt', header=None)[0].values\n","\n","le = LabelEncoder()\n","train_data[1] = le.fit_transform(train_data[1])\n","\n","train_data['text'] = train_data[0]\n","train_data['label'] = train_data[1]\n","train_data.drop(columns=[0, 1], inplace=True)\n","\n","test_data['text'] = test_data[0]\n","test_data.drop(columns=[0], inplace=True)"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["corpus = train_data['text']\n","\n","texts = []\n","\n","for i in range(len(corpus)):\n","    content = ''.join(corpus.iloc[i])\n","    words = jieba.lcut(content)\n","    words = [word for word in words if word not in cn_stopwords]\n","    texts.append(words)    \n","\n","# Count word frequencies\n","from collections import defaultdict\n","frequency = defaultdict(int)\n","for text in texts:\n","    for token in text:\n","        frequency[token] += 1\n","\n","processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]"]},{"cell_type":"code","execution_count":148,"metadata":{},"outputs":[],"source":["vocab_dim = 200\n","hidden_dim = 50\n","output_dim = 12\n","dropout = 0.1\n","batch_size = 32\n","num_layers = 1\n","max_length = 20"]},{"cell_type":"code","execution_count":149,"metadata":{},"outputs":[{"data":{"text/plain":["(590231, 670700)"]},"execution_count":149,"metadata":{},"output_type":"execute_result"}],"source":["word2vector_model = Word2Vec.load(\"word2vec_model.model\")\n","word2vector_model.train(processed_corpus, total_examples=len(processed_corpus), epochs=10)"]},{"cell_type":"code","execution_count":150,"metadata":{},"outputs":[],"source":["def create_dictionaries(model, corpus, max_length):\n","    gensim_dict = Dictionary()\n","    gensim_dict.doc2bow(model.wv.index_to_key, allow_update=True)\n","    w2indx = {v: k+1 for k, v in gensim_dict.items()}\n","    w2vec = {word: model.wv[word] for word in w2indx.keys()}\n","    \n","    def parse_dataset(corpus):\n","        data = []\n","        for sentence in corpus:\n","            new_txt = []\n","            for word in sentence:\n","                try:\n","                    new_txt.append(w2indx[word])\n","                except:\n","                    new_txt.append(0)\n","            new_txt = torch.tensor(new_txt)\n","            data.append(new_txt)\n","        return data\n","    \n","    corpus = parse_dataset(corpus)\n","    corpus = pad_sequence(corpus)[:max_length, :]\n","    return w2indx, w2vec, corpus"]},{"cell_type":"code","execution_count":151,"metadata":{},"outputs":[],"source":["def word2vec_train(corpus):\n","    index_dict, word_vectors, conbinds = create_dictionaries(model=word2vector_model, corpus=corpus, max_length=max_length)\n","    return index_dict, word_vectors, conbinds"]},{"cell_type":"code","execution_count":152,"metadata":{},"outputs":[],"source":["def get_data(index_dict, word_vectors, corpus, y):\n","    n_symbols = len(index_dict) + 1\n","    embedding_weight = np.zeros((n_symbols, vocab_dim))\n","    \n","    for word, index in index_dict.items():\n","        embedding_weight[index, :] = word_vectors[word]\n","    X_train, X_val, y_train, y_val = train_test_split(corpus, y, test_size=0.2)\n","    \n","    return n_symbols, embedding_weight, X_train, X_val, y_train, y_val"]},{"cell_type":"code","execution_count":153,"metadata":{},"outputs":[],"source":["index_dict, word_vectors, conbinds = word2vec_train(processed_corpus)\n","vocab_size, embedding_weight, X_train, X_val, y_train, y_val = get_data(index_dict, word_vectors, conbinds.T, train_data['label'])"]},{"cell_type":"code","execution_count":154,"metadata":{},"outputs":[],"source":["def convert_hidden_shape(hidden, batch_size):\n","    tensor_list = []\n","\n","    for i in range(batch_size):\n","        ts = hidden[i,: , :].reshape(1, -1)\n","        tensor_list.append(ts)\n","\n","    ts = torch.cat(tensor_list)\n","    return ts"]},{"cell_type":"code","execution_count":155,"metadata":{},"outputs":[],"source":["# Define a custom LSTM model\n","class LSTMmodel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_size, dropout, output_size, num_layers, max_length):\n","        super().__init__()\n","        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n","        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, dropout=dropout, num_layers=num_layers, batch_first=True)\n","        self.fc1 = nn.Linear(hidden_size * max_length, 128)\n","        self.fc2 = nn.Linear(128, output_size)\n","        self.dropout = nn.Dropout(dropout)\n","        self.relu = nn.ReLU()\n","    \n","    def forward(self, x):\n","        out = self.embedding(x)\n","        out, _ = self.lstm(out)\n","        out = convert_hidden_shape(out, out.shape[0])\n","        out = self.relu(out)\n","        out = self.dropout(out)\n","        out = self.fc1(out)\n","        out = self.relu(out)\n","        out = self.fc2(out)\n","        \n","        return out"]},{"cell_type":"code","execution_count":156,"metadata":{},"outputs":[],"source":["model = LSTMmodel(vocab_size, vocab_dim, hidden_dim, dropout, output_dim, num_layers=num_layers, max_length=max_length)"]},{"cell_type":"code","execution_count":157,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [-1.1459,  0.4482,  0.5943,  ..., -0.2281,  0.3203,  0.2932],\n","        [-0.0192,  0.0020,  0.0376,  ..., -0.0782, -0.0137, -0.0263],\n","        ...,\n","        [-0.0625, -0.0014,  0.0504,  ..., -0.0599,  0.0166,  0.0681],\n","        [-0.2156,  0.1499, -0.5560,  ..., -0.2428, -1.3503, -0.0564],\n","        [ 0.3185, -0.0058, -0.2687,  ..., -0.3950, -0.8556,  0.6132]])"]},"execution_count":157,"metadata":{},"output_type":"execute_result"}],"source":["model.embedding.weight.data.copy_(torch.from_numpy(embedding_weight))"]},{"cell_type":"code","execution_count":158,"metadata":{},"outputs":[],"source":["optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","criterion = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":159,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 0.01570174281662407 Train Accuracy:0.9360537190082645 Validation Accuracy: 0.8966942148760331\n","Epoch 2, Loss: 0.007125678287062809 Train Accuracy:0.9574380165289256 Validation Accuracy: 0.8946280991735537\n","Epoch 3, Loss: 0.004594638801472308 Train Accuracy:0.9630165289256198 Validation Accuracy: 0.8896694214876033\n","Epoch 4, Loss: 0.003986751025128669 Train Accuracy:0.9727272727272728 Validation Accuracy: 0.8909090909090909\n","Epoch 5, Loss: 0.003563374232316698 Train Accuracy:0.9727272727272728 Validation Accuracy: 0.8805785123966943\n","Epoch 6, Loss: 0.0030818681416604696 Train Accuracy:0.9808884297520661 Validation Accuracy: 0.8830578512396694\n","Epoch 7, Loss: 0.002454325674465843 Train Accuracy:0.984504132231405 Validation Accuracy: 0.890495867768595\n","Epoch 8, Loss: 0.0018688649398849613 Train Accuracy:0.9865702479338843 Validation Accuracy: 0.8876033057851239\n","Epoch 9, Loss: 0.002112989450033923 Train Accuracy:0.9869834710743801 Validation Accuracy: 0.8921487603305785\n","Epoch 10, Loss: 0.0033062433149725697 Train Accuracy:0.9815082644628099 Validation Accuracy: 0.8855371900826446\n","Epoch 11, Loss: 0.003068084167660219 Train Accuracy:0.9854338842975207 Validation Accuracy: 0.8896694214876033\n","Epoch 12, Loss: 0.0023919944422528183 Train Accuracy:0.9831611570247933 Validation Accuracy: 0.8797520661157024\n","Epoch 13, Loss: 0.003185257989522774 Train Accuracy:0.9797520661157024 Validation Accuracy: 0.8706611570247934\n","Epoch 14, Loss: 0.0026505951110753017 Train Accuracy:0.9871900826446282 Validation Accuracy: 0.8760330578512396\n","Epoch 15, Loss: 0.002011722245093307 Train Accuracy:0.9852272727272727 Validation Accuracy: 0.8760330578512396\n","Validation Accuracy: 0.8760330578512396\n"]}],"source":["y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n","y_val_tensor = torch.tensor(y_val.values, dtype=torch.long)\n","# 训练模型\n","\n","for epoch in range(15):\n","    model.train()\n","    total_loss = 0\n","    for i in range(0, len(X_train) - batch_size, batch_size):\n","        optimizer.zero_grad()\n","        x_batch = X_train[i:i+batch_size]\n","        y_batch_tensor = y_train_tensor[i:i+batch_size]\n","        outputs = model(x_batch)\n","        loss = criterion(outputs, y_batch_tensor)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    \n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model(X_train)\n","        _, predicted = torch.max(outputs.data, 1)\n","        train_accuracy = (predicted == y_train_tensor).sum().item() / len(y_train_tensor)\n","        \n","        \n","        outputs = model(X_val)\n","        _, predicted = torch.max(outputs.data, 1)\n","        accuracy = (predicted == y_val_tensor).sum().item() / len(y_val_tensor)\n","        \n","        print(f'Epoch {epoch+1}, Loss: {total_loss / X_train.shape[0]}', f\"Train Accuracy:{train_accuracy}\", f'Validation Accuracy: {accuracy}')\n","        \n","model.eval()\n","with torch.no_grad():\n","    outputs = model(X_val)\n","    _, predicted = torch.max(outputs.data, 1)\n","    accuracy = (predicted == y_val_tensor).sum().item() / len(y_val_tensor)\n","    print(f'Validation Accuracy: {accuracy}')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
